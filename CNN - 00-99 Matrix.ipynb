{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib._util.visualplot as vp\n",
    "import lib._util.fileproc as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Time measurement\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Sound notification\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_CODE      = 'MAG'\n",
    "TARGET            = 'target4'\n",
    "SOURCE_PATH_TRANS = f'resources/output/eda_trans/file/{COMPANY_CODE}/'\n",
    "OUT_PATH_GRAPH    = f'resources/output/cnn_00-99/graph/{COMPANY_CODE}/'\n",
    "OUT_PATH_FILE     = f'resources/output/cnn_00-99/file/{COMPANY_CODE}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_taken(seconds):\n",
    "    print(f'\\nTime Taken: {str(timedelta(seconds=seconds))}')\n",
    "    winsound.Beep(frequency=1000, duration=100)\n",
    "    winsound.Beep(frequency=1500, duration=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Feature Loading\n",
    "- Load digit frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(filename):\n",
    "    source_file = f'{SOURCE_PATH_TRANS}{filename}'\n",
    "    df_chunks   = pd.read_csv(source_file, sep=';',\n",
    "                              usecols=['draw_date', 'draw_period'] + [str(x).zfill(2) for x in range(100)],\n",
    "                              parse_dates=['draw_date'],\n",
    "                              date_parser=lambda x: pd.to_datetime(x, format='%Y-%m-%d'),\n",
    "                              chunksize=50_000)\n",
    "    return pd.concat(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = load_feature(f'{COMPANY_CODE} - digit_frequency.csv')\n",
    "\n",
    "vp.faststat(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Target Loading\n",
    "- Create target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_target(filename):\n",
    "    source_file = f'{SOURCE_PATH_TRANS}{filename}'\n",
    "    df_chunks   = pd.read_csv(source_file, sep=';',\n",
    "                              usecols=['draw_date', 'draw_period', '1st'],\n",
    "                              dtype={'1st': str},\n",
    "                              parse_dates=['draw_date'],\n",
    "                              date_parser=lambda x: pd.to_datetime(x, format='%Y-%m-%d'),\n",
    "                              chunksize=50_000)\n",
    "    return pd.concat(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = load_target(f'{COMPANY_CODE} - transactions.csv')\n",
    "\n",
    "vp.faststat(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take target from following period\n",
    "target_df['target'] = target_df['1st'].shift(-1)\n",
    "\n",
    "# Split target into digits\n",
    "for index in [x for x in range(4)]:\n",
    "    column = f'target{4 - index}'\n",
    "    target_df[column] = target_df['target'].apply(lambda x: x[index] if x == x else x)\n",
    "    target_df[column] = target_df[column].astype(float).astype('Int8')\n",
    "\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.drop(columns=['1st', 'target'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - Dataset\n",
    "- Map target label to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.shape, target_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = feature_df.merge(target_df, on=['draw_date', 'draw_period'], how='inner')\n",
    "\n",
    "vp.faststat(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_dist(df, column):\n",
    "    count_df = df[column].value_counts().to_frame(name='Count')\n",
    "    ratio_df = df[column].value_counts(normalize=True).to_frame(name='Ratio')\n",
    "    \n",
    "    dist_df  = count_df.merge(ratio_df, left_index=True, right_index=True, how='left')\n",
    "    print(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "data_df.dropna(inplace=True)\n",
    "\n",
    "print('Full dataset:')\n",
    "val_dist(data_df, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_target(df, target, n_remain, excludes=[]):\n",
    "    np.random.seed(10000)\n",
    "    \n",
    "    dfs = []\n",
    "    for target_label in np.unique(df[target]):\n",
    "        indexes = df[df[target] == target_label].index\n",
    "        indexes = [x for x in indexes if x not in excludes]\n",
    "        \n",
    "        choices = np.random.choice(indexes, size=n_remain, replace=False)\n",
    "        dfs.append(df[df.index.isin(choices)].copy())\n",
    "        \n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train & validation dataset with balanced target label\n",
    "train_df = balanced_target(data_df, target=TARGET, n_remain=350)\n",
    "valid_df = balanced_target(data_df, target=TARGET, n_remain=150, excludes=train_df.index)\n",
    "\n",
    "# Remaining goes to test dataset\n",
    "used_indexes = list(train_df.index) + list(valid_df.index)\n",
    "test_df      = data_df[~data_df.index.isin(used_indexes)].copy()\n",
    "\n",
    "# Shuffle dataset\n",
    "train_df = train_df.sample(frac=1, random_state=0)\n",
    "valid_df = valid_df.sample(frac=1, random_state=0)\n",
    "test_df  = test_df.sample(frac=1, random_state=0)\n",
    "\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:')\n",
    "val_dist(train_df, TARGET)\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "val_dist(valid_df, TARGET)\n",
    "\n",
    "print('\\nTest dataset:')\n",
    "val_dist(test_df, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_period(df, title):\n",
    "    sample_df = df.copy()\n",
    "    sample_df['year_month'] = sample_df['draw_date'].dt.to_period('M').astype(str)\n",
    "    sample_df = sample_df.groupby(['dataset', 'year_month']).agg(\n",
    "        count=('year_month', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    fig = px.bar(sample_df, x='year_month', y='count', facet_row='dataset')\n",
    "    vp.generate_plot(fig,\n",
    "                     out_path=OUT_PATH_GRAPH,\n",
    "                     out_filename=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['dataset'] = 'train'\n",
    "valid_df['dataset'] = 'validate'\n",
    "test_df['dataset']  = 'test'\n",
    "\n",
    "sampling_period(pd.concat([train_df, valid_df, test_df]),\n",
    "                title='Phase 3 - Bar - Draw Date (Sample)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 - Classification\n",
    "- Separate dataset to features & target\n",
    "- Feature scaling\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X_train = train_df[[str(x).zfill(2) for x in range(100)]].values.reshape(-1, 10, 10)\n",
    "X_valid = valid_df[[str(x).zfill(2) for x in range(100)]].values.reshape(-1, 10, 10)\n",
    "X_test  = test_df[[str(x).zfill(2) for x in range(100)]].values.reshape(-1, 10, 10)\n",
    "\n",
    "y_train = train_df[TARGET]\n",
    "y_valid = valid_df[TARGET]\n",
    "y_test  = test_df[TARGET]\n",
    "\n",
    "print('Train dataset:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "\n",
    "print('\\nTest dataset:')\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X):\n",
    "    new_X = []\n",
    "    \n",
    "    # NOTE: Normalize each matrix to range from 0 - 1 individually\n",
    "    for x in X:\n",
    "        _min  = np.amin(x)\n",
    "        _max  = np.amax(x)\n",
    "        new_x = (x - _min) / (_max - _min)\n",
    "        new_X.append(new_x)\n",
    "        \n",
    "    return np.array(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_scaling(X_train)\n",
    "X_valid = feature_scaling(X_valid)\n",
    "X_test  = feature_scaling(X_test)\n",
    "\n",
    "print('Train dataset:')\n",
    "print(np.amin(X_train), np.amax(X_train))\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "print(np.amin(X_valid), np.amax(X_valid))\n",
    "\n",
    "print('\\nTest dataset:')\n",
    "print(np.amin(X_test), np.amax(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - create CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
