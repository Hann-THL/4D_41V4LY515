{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib._util.visualplot as vp\n",
    "import lib._util.fileproc as fp\n",
    "\n",
    "# Feature encoding\n",
    "from lib._class.DFOneHotEncoder import DFOneHotEncoder\n",
    "\n",
    "# Feature scaling\n",
    "from lib._class.DFMinMaxScaler import DFMinMaxScaler\n",
    "\n",
    "# Feature extraction\n",
    "from lib._class.DFIvis import DFIvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.colors import DEFAULT_PLOTLY_COLORS\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, CategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_CODE      = 'MAG'\n",
    "TARGET            = 'target_d4'\n",
    "SOURCE_PATH_TRANS = f'resources/output/eda_trans/file/{COMPANY_CODE}/'\n",
    "OUT_PATH_GRAPH    = f'resources/output/ann_digit_trans/graph/{COMPANY_CODE}/'\n",
    "OUT_PATH_FILE     = f'resources/output/ann_digit_trans/file/{COMPANY_CODE}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Feature Loading\n",
    "- Load digit frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(filename):\n",
    "    source_file = f'{SOURCE_PATH_TRANS}{filename}'\n",
    "    df_chunks   = pd.read_csv(source_file, sep=';',\n",
    "                              parse_dates=['draw_date'],\n",
    "                              date_parser=lambda x: pd.to_datetime(x, format='%Y-%m-%d'),\n",
    "                              chunksize=50_000)\n",
    "    return pd.concat(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = load_feature(f'{COMPANY_CODE} - digit_frequency.csv')\n",
    "\n",
    "vp.faststat(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Target Loading\n",
    "- Create target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_target(filename):\n",
    "    source_file = f'{SOURCE_PATH_TRANS}{filename}'\n",
    "    df_chunks   = pd.read_csv(source_file, sep=';',\n",
    "                              dtype={x: str for x in ['1st', '2nd', '3rd'] + \\\n",
    "                                                     [f'Sp{x +1}' for x in range(10)] + \\\n",
    "                                                     [f'Cons{x +1}' for x in range(10)]},\n",
    "                              parse_dates=['draw_date'],\n",
    "                              date_parser=lambda x: pd.to_datetime(x, format='%Y-%m-%d'),\n",
    "                              chunksize=50_000)\n",
    "    return pd.concat(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = load_target(f'{COMPANY_CODE} - transactions.csv')\n",
    "\n",
    "vp.faststat(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take target from following period\n",
    "target_df['target'] = target_df['1st'].shift(-1)\n",
    "\n",
    "# Split price numbers & target into digits\n",
    "categories = ['1st', '2nd', '3rd'] + [f'Sp{x +1}' for x in range(10)] + [f'Cons{x +1}' for x in range(10)]\n",
    "for column in categories + ['target']:\n",
    "    for index in [x for x in range(4)]:\n",
    "        new_column = f'{column}_d{4 - index}'\n",
    "        target_df[new_column] = target_df[column].apply(lambda x: x[index] if x == x else x)\n",
    "\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.drop(columns=categories + ['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - Dataset\n",
    "- Map target label to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.shape, target_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = feature_df.merge(target_df, on=['draw_date', 'draw_period'], how='inner')\n",
    "\n",
    "vp.faststat(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.dropna(inplace=True)\n",
    "\n",
    "columns = [x for x in data_df.columns if any([x.startswith(y) for y in categories + ['target']])]\n",
    "data_df[columns] = data_df[columns].astype(int)\n",
    "\n",
    "# Target distribution\n",
    "print('Full dataset:')\n",
    "vp.value_count(data_df, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_target(df, target, n_remain, excludes=[], random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    dfs = []\n",
    "    for target_label in np.unique(df[target]):\n",
    "        indexes = df[df[target] == target_label].index\n",
    "        indexes = [x for x in indexes if x not in excludes]\n",
    "        \n",
    "        choices = np.random.choice(indexes, size=n_remain, replace=False)\n",
    "        dfs.append(df[df.index.isin(choices)].copy())\n",
    "        \n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train dataset with balanced target label\n",
    "train_df = balanced_target(data_df, target=TARGET, n_remain=500, random_state=10000)\n",
    "\n",
    "# Remaining goes to test dataset\n",
    "used_indexes = list(train_df.index)\n",
    "test_df      = data_df[~data_df.index.isin(used_indexes)].copy()\n",
    "\n",
    "# Shuffle dataset\n",
    "train_df = train_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "test_df  = test_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:')\n",
    "vp.value_count(train_df, TARGET)\n",
    "\n",
    "print('\\nTest dataset:')\n",
    "vp.value_count(test_df, TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_period(df, title):\n",
    "    sample_df = df.copy()\n",
    "    sample_df['year_month'] = sample_df['draw_date'].dt.to_period('M').astype(str)\n",
    "    sample_df = sample_df.groupby(['dataset', 'year_month']).agg(\n",
    "        count=('year_month', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    fig = px.bar(sample_df, x='year_month', y='count', facet_row='dataset')\n",
    "    fig.update_layout(title=title)\n",
    "    vp.generate_plot(fig,\n",
    "                     out_path=OUT_PATH_GRAPH,\n",
    "                     out_filename=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['dataset'] = 'train'\n",
    "test_df['dataset']  = 'test'\n",
    "\n",
    "sampling_period(pd.concat([train_df, test_df]),\n",
    "                title='Phase 2 - Bar - Draw Date (Sample)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 - Feature Extraction\n",
    "- Separate dataset to features & target\n",
    "- Feature encoding\n",
    "- Ivis dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_target_split(df):\n",
    "    X = df[[x for x in train_df.columns\n",
    "            if all([not x.startswith(y) for y in ['draw_date', 'draw_period', 'dataset', 'target']])]]\n",
    "    y = df[TARGET]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X_train, y_train = feature_target_split(train_df)\n",
    "X_test,  y_test  = feature_target_split(test_df)\n",
    "\n",
    "print('Train dataset:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "print('\\nTest dataset:')\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary crossentropy target\n",
    "target_onehot_encoder = DFOneHotEncoder(dtype='byte')\n",
    "y_train_binary = target_onehot_encoder.fit_transform(y_train.to_frame())\n",
    "\n",
    "# Sparse categorical crossentropy target\n",
    "y_train_sparse = y_train.copy()\n",
    "\n",
    "y_train_binary.shape, y_train_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "onehot_columns = [x for x in X_train.columns if any([x.startswith(y) for y in categories])]\n",
    "onehot_encoder = DFOneHotEncoder(columns=onehot_columns, dtype='byte', drop='first')\n",
    "\n",
    "minmax_scaler  = DFMinMaxScaler(columns=[x for x in X_train.columns if x not in onehot_columns])\n",
    "\n",
    "# TODO - define own model to prevent overfitting & explosive stacked_triplets_loss\n",
    "ivis_binary    = DFIvis(embedding_dims=2, epochs=100,\n",
    "                        k=15, n_epochs_without_progress=10, model='szubert',\n",
    "                        supervision_weight=1, supervision_metric='binary_crossentropy', distance='pn')\n",
    "\n",
    "ivis_sparse    = DFIvis(embedding_dims=2, epochs=100,\n",
    "                        k=15, n_epochs_without_progress=10, model='szubert',\n",
    "                        supervision_weight=1, supervision_metric='sparse_categorical_crossentropy', distance='pn')\n",
    "\n",
    "# Binary crossentropy\n",
    "steps = [\n",
    "    ('onehot_encoder', onehot_encoder),\n",
    "    ('minmax_scaler', minmax_scaler),\n",
    "    ('ivis_binary', ivis_binary),\n",
    "]\n",
    "ivis_binary_pipeline = Pipeline(steps, verbose=True)\n",
    "\n",
    "X_train_binary = ivis_binary_pipeline.fit_transform(X_train, y_train_binary)\n",
    "X_test_binary  = ivis_binary_pipeline.transform(X_test)\n",
    "\n",
    "X_train_binary.shape, X_test_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sparse categorical crossentropy\n",
    "steps = [\n",
    "    ('onehot_encoder', onehot_encoder),\n",
    "    ('minmax_scaler', minmax_scaler),\n",
    "    ('ivis_sparse', ivis_sparse),\n",
    "]\n",
    "ivis_sparse_pipeline = Pipeline(steps, verbose=True)\n",
    "\n",
    "X_train_sparse = ivis_sparse_pipeline.fit_transform(X_train, y_train_sparse)\n",
    "X_test_sparse  = ivis_sparse_pipeline.transform(X_test)\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names      = ['binary_loss', 'sparse_loss']\n",
    "losses     = [ivis_binary.model.loss_history_, ivis_sparse.model.loss_history_]\n",
    "max_length = max([len(x) for x in losses])\n",
    "\n",
    "tmp_df = pd.DataFrame([x+1 for x in range(max_length)], columns=['epoch'])\n",
    "for index, loss in enumerate(losses):\n",
    "    loss += [np.nan] * (max_length - len(loss))\n",
    "    tmp_df[names[index]] = loss\n",
    "\n",
    "vp.line(tmp_df,\n",
    "        xy_tuples=[('epoch', x) for x in names],\n",
    "        title='Phase 3 - Line - Ivis Loss',\n",
    "        out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title  = 'Phase 3 - Scatter - Ivis - Binary'\n",
    "tmp_df = pd.concat([X_train_binary, pd.Series(np.argmax(y_train_binary.values, axis=1), name=TARGET).astype(str)], axis=1)\n",
    "\n",
    "fig = px.scatter(\n",
    "    tmp_df.sort_values(by=TARGET),\n",
    "    x='ivis_0',\n",
    "    y='ivis_1',\n",
    "    color=TARGET,\n",
    "    marginal_x='histogram'\n",
    ")\n",
    "fig.update_layout(title=title)\n",
    "\n",
    "vp.generate_plot(fig,\n",
    "                 out_path=OUT_PATH_GRAPH,\n",
    "                 out_filename=title)\n",
    "\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title  = 'Phase 3 - Scatter - Ivis - Sparse Categorical'\n",
    "tmp_df = pd.concat([X_train_sparse, y_train_sparse.astype(str)], axis=1)\n",
    "\n",
    "fig = px.scatter(\n",
    "    tmp_df.sort_values(by=TARGET),\n",
    "    x='ivis_0',\n",
    "    y='ivis_1',\n",
    "    color=TARGET,\n",
    "    marginal_x='histogram'\n",
    ")\n",
    "fig.update_layout(title=title)\n",
    "\n",
    "vp.generate_plot(fig,\n",
    "                 out_path=OUT_PATH_GRAPH,\n",
    "                 out_filename=title)\n",
    "\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title  = 'Phase 3 - Scatter - Ivis - Binary (Test dataset)'\n",
    "tmp_df = pd.concat([X_test_binary, y_test.astype(str)], axis=1)\n",
    "\n",
    "fig = px.scatter(\n",
    "    tmp_df.sort_values(by=TARGET),\n",
    "    x='ivis_0',\n",
    "    y='ivis_1',\n",
    "    color=TARGET,\n",
    "    marginal_x='histogram'\n",
    ")\n",
    "fig.update_layout(title=title)\n",
    "\n",
    "vp.generate_plot(fig,\n",
    "                 out_path=OUT_PATH_GRAPH,\n",
    "                 out_filename=title)\n",
    "\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title  = 'Phase 3 - Scatter - Ivis - Sparse Categorical (Test dataset)'\n",
    "tmp_df = pd.concat([X_test_sparse, y_test.astype(str)], axis=1)\n",
    "\n",
    "fig = px.scatter(\n",
    "    tmp_df.sort_values(by=TARGET),\n",
    "    x='ivis_0',\n",
    "    y='ivis_1',\n",
    "    color=TARGET,\n",
    "    marginal_x='histogram'\n",
    ")\n",
    "fig.update_layout(title=title)\n",
    "\n",
    "vp.generate_plot(fig,\n",
    "                 out_path=OUT_PATH_GRAPH,\n",
    "                 out_filename=title)\n",
    "\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5 - Classification\n",
    "- Binary crossentropy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train & validation dataset with balanced target label\n",
    "train_df = balanced_target(pd.concat([X_train_binary, y_train], axis=1),\n",
    "                           target=TARGET, n_remain=350, random_state=10000)\n",
    "valid_df = balanced_target(pd.concat([X_train_binary, y_train], axis=1),\n",
    "                           target=TARGET, n_remain=150, random_state=10000, excludes=train_df.index)\n",
    "\n",
    "# Shuffle dataset\n",
    "train_df = train_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "valid_df = valid_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:')\n",
    "vp.value_count(train_df, TARGET)\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "vp.value_count(valid_df, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X_train_binary, y_train_binary = feature_target_split(train_df)\n",
    "X_valid_binary, y_valid_binary = feature_target_split(valid_df)\n",
    "\n",
    "# Categorical crossentropy target\n",
    "y_train_binary = target_onehot_encoder.transform(y_train_binary.to_frame())\n",
    "y_valid_binary = target_onehot_encoder.transform(y_valid_binary.to_frame())\n",
    "y_test_binary  = target_onehot_encoder.transform(y_test.to_frame())\n",
    "\n",
    "print('Train dataset:')\n",
    "print(X_train_binary.shape, y_train_binary.shape)\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "print(X_valid_binary.shape, y_valid_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "minmax_scaler  = DFMinMaxScaler()\n",
    "X_train_binary = minmax_scaler.fit_transform(X_train_binary)\n",
    "X_valid_binary = minmax_scaler.transform(X_valid_binary)\n",
    "X_test_binary  = minmax_scaler.transform(X_test_binary)\n",
    "\n",
    "X_train_binary.shape, X_valid_binary.shape, X_test_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(X):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=X.shape[1],\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    kernel_constraint=max_norm(5),\n",
    "                    use_bias=False))\n",
    "    model.add(BatchNormalization(scale=False,\n",
    "                                 renorm=True,\n",
    "                                 renorm_clipping={ 'rmax': 1, 'rmin': 0, 'dmax': 0 }))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=.2))\n",
    "    \n",
    "    model.add(Dense(64,\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    kernel_constraint=max_norm(5),\n",
    "                    use_bias=False))\n",
    "    model.add(BatchNormalization(scale=False,\n",
    "                                 renorm=True,\n",
    "                                 renorm_clipping={ 'rmax': 1, 'rmin': 0, 'dmax': 0 }))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=.2))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Referecence: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "    metrics = [\n",
    "        CategoricalAccuracy(name='categorical_accuracy'),\n",
    "        AUC(name='auc'),\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall'),\n",
    "    ]\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=.001, epsilon=.00001),\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "lrate = ReduceLROnPlateau(monitor='val_auc', factor=0.95, patience=15)\n",
    "\n",
    "# Reference: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=30, restore_best_weights=True)\n",
    "\n",
    "model   = compile_model(X_train_binary)\n",
    "history = model.fit(X_train_binary, y_train_binary,\n",
    "                    validation_data=(X_valid_binary, y_valid_binary),\n",
    "                    epochs=100,\n",
    "                    callbacks=[lrate, es],\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classif(y_true, y_pred):\n",
    "    cofmat_df = pd.DataFrame(confusion_matrix(y_true, y_pred))\n",
    "    cofmat_df.index.name   = 'True'\n",
    "    cofmat_df.columns.name = 'Pred'\n",
    "\n",
    "    print(cofmat_df)\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_test_binary.values, axis=1),\n",
    "    np.argmax(model.predict(X_test_binary), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_train_binary.values, axis=1),\n",
    "    np.argmax(model.predict(X_train_binary), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_valid_binary.values, axis=1),\n",
    "    np.argmax(model.predict(X_valid_binary), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['lr'],\n",
    "    mode='lines',\n",
    "    name='LR',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[-1]},\n",
    "))\n",
    "fig1 = go.Figure(data=data)\n",
    "\n",
    "# Loss\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['loss'],\n",
    "    mode='lines',\n",
    "    name='loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_loss'],\n",
    "    mode='lines',\n",
    "    name='val_loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig2 = go.Figure(data=data)\n",
    "\n",
    "# Accuracy\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='val_accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig3 = go.Figure(data=data)\n",
    "\n",
    "# AUC\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['auc'],\n",
    "    mode='lines',\n",
    "    name='auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_auc'],\n",
    "    mode='lines',\n",
    "    name='val_auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig4 = go.Figure(data=data)\n",
    "\n",
    "# Precision\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['precision'],\n",
    "    mode='lines',\n",
    "    name='precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_precision'],\n",
    "    mode='lines',\n",
    "    name='val_precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig5 = go.Figure(data=data)\n",
    "\n",
    "# Recall\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['recall'],\n",
    "    mode='lines',\n",
    "    name='recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_recall'],\n",
    "    mode='lines',\n",
    "    name='val_recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig6 = go.Figure(data=data)\n",
    "\n",
    "data_groups = [fig1['data'], fig2['data'], fig3['data'], fig4['data'], fig5['data'], fig6['data']]\n",
    "vp.datagroups_subplots(data_groups,\n",
    "                       max_col=3,\n",
    "                       title='Phase 5 - Metrics - Ivis - Binary',\n",
    "                       out_path=OUT_PATH_GRAPH,\n",
    "                       subplot_titles=['Learning Rate', 'Loss', 'Accuracy', 'AUC', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6 - Classification\n",
    "- Sparse categorical crossentropy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train & validation dataset with balanced target label\n",
    "train_df = balanced_target(pd.concat([X_train_sparse, y_train], axis=1),\n",
    "                           target=TARGET, n_remain=350, random_state=10000)\n",
    "valid_df = balanced_target(pd.concat([X_train_sparse, y_train], axis=1),\n",
    "                           target=TARGET, n_remain=150, random_state=10000, excludes=train_df.index)\n",
    "\n",
    "# Shuffle dataset\n",
    "train_df = train_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "valid_df = valid_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:')\n",
    "vp.value_count(train_df, TARGET)\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "vp.value_count(valid_df, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X_train_sparse, y_train_sparse = feature_target_split(train_df)\n",
    "X_valid_sparse, y_valid_sparse = feature_target_split(valid_df)\n",
    "\n",
    "# Categorical crossentropy target\n",
    "y_train_sparse = target_onehot_encoder.transform(y_train_sparse.to_frame())\n",
    "y_valid_sparse = target_onehot_encoder.transform(y_valid_sparse.to_frame())\n",
    "y_test_sparse  = target_onehot_encoder.transform(y_test.to_frame())\n",
    "\n",
    "print('Train dataset:')\n",
    "print(X_train_sparse.shape, y_train_sparse.shape)\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "print(X_valid_sparse.shape, y_valid_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "minmax_scaler  = DFMinMaxScaler()\n",
    "X_train_sparse = minmax_scaler.fit_transform(X_train_sparse)\n",
    "X_valid_sparse = minmax_scaler.transform(X_valid_sparse)\n",
    "X_test_sparse  = minmax_scaler.transform(X_test_sparse)\n",
    "\n",
    "X_train_sparse.shape, X_valid_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "lrate = ReduceLROnPlateau(monitor='val_auc', factor=0.95, patience=15)\n",
    "\n",
    "# Reference: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=30, restore_best_weights=True)\n",
    "\n",
    "model   = compile_model(X_train_sparse)\n",
    "history = model.fit(X_train_sparse, y_train_sparse,\n",
    "                    validation_data=(X_valid_sparse, y_valid_sparse),\n",
    "                    epochs=100,\n",
    "                    callbacks=[lrate, es],\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_test_sparse.values, axis=1),\n",
    "    np.argmax(model.predict(X_test_sparse), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_train_sparse.values, axis=1),\n",
    "    np.argmax(model.predict(X_train_sparse), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_valid_sparse.values, axis=1),\n",
    "    np.argmax(model.predict(X_valid_sparse), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['lr'],\n",
    "    mode='lines',\n",
    "    name='LR',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[-1]},\n",
    "))\n",
    "fig1 = go.Figure(data=data)\n",
    "\n",
    "# Loss\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['loss'],\n",
    "    mode='lines',\n",
    "    name='loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_loss'],\n",
    "    mode='lines',\n",
    "    name='val_loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig2 = go.Figure(data=data)\n",
    "\n",
    "# Accuracy\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='val_accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig3 = go.Figure(data=data)\n",
    "\n",
    "# AUC\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['auc'],\n",
    "    mode='lines',\n",
    "    name='auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_auc'],\n",
    "    mode='lines',\n",
    "    name='val_auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig4 = go.Figure(data=data)\n",
    "\n",
    "# Precision\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['precision'],\n",
    "    mode='lines',\n",
    "    name='precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_precision'],\n",
    "    mode='lines',\n",
    "    name='val_precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig5 = go.Figure(data=data)\n",
    "\n",
    "# Recall\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['recall'],\n",
    "    mode='lines',\n",
    "    name='recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_recall'],\n",
    "    mode='lines',\n",
    "    name='val_recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig6 = go.Figure(data=data)\n",
    "\n",
    "data_groups = [fig1['data'], fig2['data'], fig3['data'], fig4['data'], fig5['data'], fig6['data']]\n",
    "vp.datagroups_subplots(data_groups,\n",
    "                       max_col=3,\n",
    "                       title='Phase 6 - Metrics - Ivis - Sparse Categorical',\n",
    "                       out_path=OUT_PATH_GRAPH,\n",
    "                       subplot_titles=['Learning Rate', 'Loss', 'Accuracy', 'AUC', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 7 - Classification\n",
    "- Binary + Sparse categorical crossentropy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combine = pd.concat([\n",
    "    X_train_binary.rename(columns={'ivis_0': 'ivis_binary_0', 'ivis_1': 'ivis_binary_1'}),\n",
    "    X_train_sparse.rename(columns={'ivis_0': 'ivis_sparse_0', 'ivis_1': 'ivis_sparse_1'}),\n",
    "], axis=1)\n",
    "\n",
    "X_valid_combine = pd.concat([\n",
    "    X_valid_binary.rename(columns={'ivis_0': 'ivis_binary_0', 'ivis_1': 'ivis_binary_1'}),\n",
    "    X_valid_sparse.rename(columns={'ivis_0': 'ivis_sparse_0', 'ivis_1': 'ivis_sparse_1'}),\n",
    "], axis=1)\n",
    "\n",
    "X_test_combine  = pd.concat([\n",
    "    X_test_binary.rename(columns={'ivis_0': 'ivis_binary_0', 'ivis_1': 'ivis_binary_1'}),\n",
    "    X_test_sparse.rename(columns={'ivis_0': 'ivis_sparse_0', 'ivis_1': 'ivis_sparse_1'}),\n",
    "], axis=1)\n",
    "\n",
    "X_train_combine.shape, X_valid_combine.shape, X_test_combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_combine = y_train_binary.copy()\n",
    "y_valid_combine = y_valid_binary.copy()\n",
    "y_test_combine  = y_test_binary.copy()\n",
    "\n",
    "y_train_combine.shape, y_valid_combine.shape, y_test_combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "lrate = ReduceLROnPlateau(monitor='val_auc', factor=0.95, patience=15)\n",
    "\n",
    "# Reference: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=30, restore_best_weights=True)\n",
    "\n",
    "model   = compile_model(X_train_combine)\n",
    "history = model.fit(X_train_combine, y_train_combine,\n",
    "                    validation_data=(X_valid_combine, y_valid_combine),\n",
    "                    epochs=100,\n",
    "                    callbacks=[lrate, es],\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_test_combine.values, axis=1),\n",
    "    np.argmax(model.predict(X_test_combine), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_train_combine.values, axis=1),\n",
    "    np.argmax(model.predict(X_train_combine), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_valid_combine.values, axis=1),\n",
    "    np.argmax(model.predict(X_valid_combine), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['lr'],\n",
    "    mode='lines',\n",
    "    name='LR',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[-1]},\n",
    "))\n",
    "fig1 = go.Figure(data=data)\n",
    "\n",
    "# Loss\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['loss'],\n",
    "    mode='lines',\n",
    "    name='loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_loss'],\n",
    "    mode='lines',\n",
    "    name='val_loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig2 = go.Figure(data=data)\n",
    "\n",
    "# Accuracy\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='val_accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig3 = go.Figure(data=data)\n",
    "\n",
    "# AUC\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['auc'],\n",
    "    mode='lines',\n",
    "    name='auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_auc'],\n",
    "    mode='lines',\n",
    "    name='val_auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig4 = go.Figure(data=data)\n",
    "\n",
    "# Precision\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['precision'],\n",
    "    mode='lines',\n",
    "    name='precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_precision'],\n",
    "    mode='lines',\n",
    "    name='val_precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig5 = go.Figure(data=data)\n",
    "\n",
    "# Recall\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['recall'],\n",
    "    mode='lines',\n",
    "    name='recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_recall'],\n",
    "    mode='lines',\n",
    "    name='val_recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig6 = go.Figure(data=data)\n",
    "\n",
    "data_groups = [fig1['data'], fig2['data'], fig3['data'], fig4['data'], fig5['data'], fig6['data']]\n",
    "vp.datagroups_subplots(data_groups,\n",
    "                       max_col=3,\n",
    "                       title='Phase 7 - Metrics - Ivis - Binary + Sparse Categorical',\n",
    "                       out_path=OUT_PATH_GRAPH,\n",
    "                       subplot_titles=['Learning Rate', 'Loss', 'Accuracy', 'AUC', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
