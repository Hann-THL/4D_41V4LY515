{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib._util.visualplot as vp\n",
    "import lib._util.fileproc as fp\n",
    "\n",
    "# Feature selection\n",
    "from lib._class.feature_selection.filtering.DFChi2Threshold import DFChi2Threshold\n",
    "from lib._class.feature_selection.filtering.DFMutualInfoClassifierThreshold import DFMutualInfoClassifierThreshold\n",
    "from lib._class.feature_selection.filtering.DFANOVAClassifierThreshold import DFANOVAClassifierThreshold\n",
    "from lib._class.feature_selection.filtering.DFROCAUCThreshold import DFROCAUCThreshold\n",
    "\n",
    "from lib._class.feature_selection.embedding.DFLogisticRegressionSelector import DFLogisticRegressionSelector\n",
    "from lib._class.feature_selection.embedding.DFRidgeClassifierSelector import DFRidgeClassifierSelector\n",
    "from lib._class.feature_selection.embedding.DFDecisionTreeClassifierSelector import DFDecisionTreeClassifierSelector\n",
    "from lib._class.feature_selection.embedding.DFExtraTreeClassifierSelector import DFExtraTreeClassifierSelector\n",
    "from lib._class.feature_selection.embedding.DFRandomForestClassifierSelector import DFRandomForestClassifierSelector\n",
    "from lib._class.feature_selection.embedding.DFXGBClassifierSelector import DFXGBClassifierSelector\n",
    "from lib._class.feature_selection.embedding.DFCatBoostClassifierSelector import DFCatBoostClassifierSelector\n",
    "\n",
    "# Feature encoding\n",
    "from lib._class.DFOneHotEncoder import DFOneHotEncoder\n",
    "\n",
    "# Feature scaling\n",
    "from lib._class.DFMinMaxScaler import DFMinMaxScaler\n",
    "\n",
    "# Feature extraction\n",
    "from lib._class.DFIvis import DFIvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.colors import DEFAULT_PLOTLY_COLORS\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, CategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_CODE      = 'MAG'\n",
    "TARGET            = 'target_d4'\n",
    "SOURCE_PATH_TRANS = f'resources/output/eda_trans/file/{COMPANY_CODE}/'\n",
    "OUT_PATH_GRAPH    = f'resources/output/ann_digit_trans/graph/{COMPANY_CODE}/'\n",
    "OUT_PATH_FILE     = f'resources/output/ann_digit_trans/file/{COMPANY_CODE}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Feature Loading\n",
    "- Load digit frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(filename):\n",
    "    source_file = f'{SOURCE_PATH_TRANS}{filename}'\n",
    "    df_chunks   = pd.read_csv(source_file, sep=';',\n",
    "                              parse_dates=['draw_date'],\n",
    "                              date_parser=lambda x: pd.to_datetime(x, format='%Y-%m-%d'),\n",
    "                              chunksize=50_000)\n",
    "    return pd.concat(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = load_feature(f'{COMPANY_CODE} - digit_frequency.csv')\n",
    "\n",
    "vp.faststat(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Target Loading\n",
    "- Create target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_target(filename):\n",
    "    source_file = f'{SOURCE_PATH_TRANS}{filename}'\n",
    "    df_chunks   = pd.read_csv(source_file, sep=';',\n",
    "                              dtype={x: str for x in ['1st', '2nd', '3rd'] + \\\n",
    "                                                     [f'Sp{x +1}' for x in range(10)] + \\\n",
    "                                                     [f'Cons{x +1}' for x in range(10)]},\n",
    "                              parse_dates=['draw_date'],\n",
    "                              date_parser=lambda x: pd.to_datetime(x, format='%Y-%m-%d'),\n",
    "                              chunksize=50_000)\n",
    "    return pd.concat(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = load_target(f'{COMPANY_CODE} - transactions.csv')\n",
    "\n",
    "vp.faststat(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take target from following period\n",
    "target_df['target'] = target_df['1st'].shift(-1)\n",
    "\n",
    "# Split price numbers & target into digits\n",
    "categories = ['1st', '2nd', '3rd'] + [f'Sp{x +1}' for x in range(10)] + [f'Cons{x +1}' for x in range(10)]\n",
    "for column in categories + ['target']:\n",
    "    for index in [x for x in range(4)]:\n",
    "        new_column = f'{column}_d{4 - index}'\n",
    "        target_df[new_column] = target_df[column].apply(lambda x: x[index] if x == x else x)\n",
    "\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.drop(columns=categories + ['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - Dataset\n",
    "- Map target label to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.shape, target_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = feature_df.merge(target_df, on=['draw_date', 'draw_period'], how='inner')\n",
    "data_df.rename(columns={x: f'_Freq_{x}'\n",
    "                        for x in [str(x) for x in range(10)] + [str(x).zfill(2) for x in range(100)]}, inplace=True)\n",
    "\n",
    "vp.faststat(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.dropna(inplace=True)\n",
    "\n",
    "columns = [x for x in data_df.columns if any([x.startswith(y) for y in categories + ['target']])]\n",
    "data_df[columns] = data_df[columns].astype(int)\n",
    "\n",
    "# Target distribution\n",
    "print('Full dataset:')\n",
    "vp.value_count(data_df, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_target(df, target, n_remain, excludes=[], random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    dfs = []\n",
    "    for target_label in np.unique(df[target]):\n",
    "        indexes = df[df[target] == target_label].index\n",
    "        indexes = [x for x in indexes if x not in excludes]\n",
    "        \n",
    "        choices = np.random.choice(indexes, size=n_remain, replace=False)\n",
    "        dfs.append(df[df.index.isin(choices)].copy())\n",
    "        \n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train dataset with balanced target label\n",
    "train_df = balanced_target(data_df, target=TARGET, n_remain=500, random_state=10000)\n",
    "\n",
    "# Remaining goes to test dataset\n",
    "used_indexes = list(train_df.index)\n",
    "test_df      = data_df[~data_df.index.isin(used_indexes)].copy()\n",
    "\n",
    "# Shuffle dataset\n",
    "train_df = train_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "test_df  = test_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:')\n",
    "vp.value_count(train_df, TARGET)\n",
    "\n",
    "print('\\nTest dataset:')\n",
    "vp.value_count(test_df, TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_period(df, title):\n",
    "    sample_df = df.copy()\n",
    "    sample_df['year_month'] = sample_df['draw_date'].dt.to_period('M').astype(str)\n",
    "    sample_df = sample_df.groupby(['dataset', 'year_month']).agg(\n",
    "        count=('year_month', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    fig = px.bar(sample_df, x='year_month', y='count', facet_row='dataset')\n",
    "    fig.update_layout(title=title)\n",
    "    vp.generate_plot(fig,\n",
    "                     out_path=OUT_PATH_GRAPH,\n",
    "                     out_filename=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['dataset'] = 'train'\n",
    "test_df['dataset']  = 'test'\n",
    "\n",
    "sampling_period(pd.concat([train_df, test_df]),\n",
    "                title='Phase 2 - Bar - Draw Date (Sample)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 - Feature Selection\n",
    "- Classification method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_target_split(df):\n",
    "    X = df[[x for x in df.columns\n",
    "            if all([not x.startswith(y) for y in ['draw_date', 'draw_period', 'dataset', 'target']])]]\n",
    "    y = df[TARGET]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X_train, y_train = feature_target_split(train_df)\n",
    "X_test,  y_test  = feature_target_split(test_df)\n",
    "\n",
    "print('Train dataset:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "print('\\nTest dataset:')\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_groups    = []\n",
    "subplot_titles = []\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info_threshold = DFMutualInfoClassifierThreshold(cv=cv)\n",
    "\n",
    "steps = [\n",
    "    ('mutual_info_threshold', mutual_info_threshold),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "mutual_info_threshold.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=mutual_info_threshold.stat_df['feature'],\n",
    "            y=mutual_info_threshold.stat_df['average_score'],\n",
    "            marker_color=np.where(mutual_info_threshold.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('Mutual Information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Univariate ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_threshold = DFANOVAClassifierThreshold(cv=cv)\n",
    "\n",
    "steps = [\n",
    "    ('anova_threshold', anova_threshold),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "anova_threshold.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=anova_threshold.stat_df['feature'],\n",
    "            y=anova_threshold.stat_df['average_score'],\n",
    "            marker_color=np.where(anova_threshold.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('Univariate ANOV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Chi Square (Ï‡2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_threshold = DFChi2Threshold(cv=cv)\n",
    "\n",
    "steps = [\n",
    "    ('chi2_threshold', chi2_threshold),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "chi2_threshold.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=chi2_threshold.stat_df['feature'],\n",
    "            y=chi2_threshold.stat_df['average_score'],\n",
    "            marker_color=np.where(chi2_threshold.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('Chi-Square')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Univariate ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rocauc_threshold = DFROCAUCThreshold(threshold=.5, cv=cv, multi_class='ovr')\n",
    "\n",
    "steps = [\n",
    "    ('rocauc_threshold', rocauc_threshold),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "rocauc_threshold.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=rocauc_threshold.stat_df['feature'],\n",
    "            y=rocauc_threshold.stat_df['average_score'],\n",
    "            marker_color=np.where(rocauc_threshold.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('Univariate ROC-AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_selector = DFLogisticRegressionSelector(cv=cv, max_iter=1_000)\n",
    "\n",
    "steps = [\n",
    "    ('logistic_selector', logistic_selector),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "logistic_selector.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=logistic_selector.stat_df['feature'],\n",
    "            y=logistic_selector.stat_df['average_score'],\n",
    "            marker_color=np.where(logistic_selector.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ridge_selector = DFRidgeClassifierSelector(cv=cv)\n",
    "\n",
    "steps = [\n",
    "    ('ridge_selector', ridge_selector),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "ridge_selector.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=ridge_selector.stat_df['feature'],\n",
    "            y=ridge_selector.stat_df['average_score'],\n",
    "            marker_color=np.where(ridge_selector.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('Ridge Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decision_tree_selector = DFDecisionTreeClassifierSelector(cv=cv)\n",
    "\n",
    "steps = [\n",
    "    ('decision_tree_selector', decision_tree_selector),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "decision_tree_selector.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=decision_tree_selector.stat_df['feature'],\n",
    "            y=decision_tree_selector.stat_df['average_score'],\n",
    "            marker_color=np.where(decision_tree_selector.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extra_tree_selector = DFExtraTreeClassifierSelector(cv=cv)\n",
    "\n",
    "steps = [\n",
    "    ('extra_tree_selector', extra_tree_selector),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "extra_tree_selector.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=extra_tree_selector.stat_df['feature'],\n",
    "            y=extra_tree_selector.stat_df['average_score'],\n",
    "            marker_color=np.where(extra_tree_selector.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('Extra Tree Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_forest_selector = DFRandomForestClassifierSelector(cv=cv)\n",
    "\n",
    "steps = [\n",
    "    ('random_forest_selector', random_forest_selector),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "random_forest_selector.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=random_forest_selector.stat_df['feature'],\n",
    "            y=random_forest_selector.stat_df['average_score'],\n",
    "            marker_color=np.where(random_forest_selector.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgboost_selector = DFXGBClassifierSelector(cv=cv)\n",
    "\n",
    "steps = [\n",
    "    ('xgboost_selector', xgboost_selector),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "xgboost_selector.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=xgboost_selector.stat_df['feature'],\n",
    "            y=xgboost_selector.stat_df['average_score'],\n",
    "            marker_color=np.where(xgboost_selector.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('XGBoost Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories        = ['1st', '2nd', '3rd'] + [f'Sp{x +1}' for x in range(10)] + [f'Cons{x +1}' for x in range(10)]\n",
    "catboost_selector = DFCatBoostClassifierSelector(\n",
    "                        cv=cv,\n",
    "                        iterations=10_000,\n",
    "                        early_stopping_rounds=200,\n",
    "                        verbose=1_000,\n",
    "                        task_type='GPU',\n",
    "                        # eval_metric='MultiClassOneVsAll',\n",
    "                        loss_function='MultiClassOneVsAll',\n",
    "                        cat_features=[x for x in X_train.columns if any([x.startswith(f'{y}_') for y in categories])]\n",
    "                    )\n",
    "\n",
    "steps = [\n",
    "    ('catboost_selector', catboost_selector),\n",
    "]\n",
    "Pipeline(steps, verbose=2).fit(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    ")\n",
    "\n",
    "# Graph data\n",
    "catboost_selector.stat_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=catboost_selector.stat_df['feature'],\n",
    "            y=catboost_selector.stat_df['average_score'],\n",
    "            marker_color=np.where(catboost_selector.stat_df['support'],\n",
    "                                  DEFAULT_PLOTLY_COLORS[2], DEFAULT_PLOTLY_COLORS[3])\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append('CatBoost Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = f'{OUT_PATH_FILE}/classification/'\n",
    "fp.generate_excel(mutual_info_threshold.stat_df,  out_path, 'mutual_info_threshold.xlsx')\n",
    "fp.generate_excel(anova_threshold.stat_df,        out_path, 'anova_threshold.xlsx')\n",
    "fp.generate_excel(chi2_threshold.stat_df,         out_path, 'chi2_threshold.xlsx')\n",
    "fp.generate_excel(rocauc_threshold.stat_df,       out_path, 'rocauc_threshold.xlsx')\n",
    "fp.generate_excel(logistic_selector.stat_df,      out_path, 'logistic_selector.xlsx')\n",
    "fp.generate_excel(ridge_selector.stat_df,         out_path, 'ridge_selector.xlsx')\n",
    "fp.generate_excel(decision_tree_selector.stat_df, out_path, 'decision_tree_selector.xlsx')\n",
    "fp.generate_excel(extra_tree_selector.stat_df,    out_path, 'extra_tree_selector.xlsx')\n",
    "fp.generate_excel(random_forest_selector.stat_df, out_path, 'random_forest_selector.xlsx')\n",
    "fp.generate_excel(xgboost_selector.stat_df,       out_path, 'xgboost_selector.xlsx')\n",
    "fp.generate_excel(catboost_selector.stat_df,      out_path, 'catboost_selector.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Top N Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile feature selection scores\n",
    "feature_df = mutual_info_threshold.stat_df.rename(columns={\n",
    "    'average_score': 'mutual_info_score',\n",
    "    'support':       'mutual_info_support'\n",
    "}).merge(\n",
    "    anova_threshold.stat_df[['feature', 'average_score', 'support']].rename(columns={\n",
    "        'average_score': 'anova_score',\n",
    "        'support':       'anova_support'\n",
    "    }), on='feature', how='left'\n",
    ").merge(\n",
    "    chi2_threshold.stat_df[['feature', 'average_score', 'support']].rename(columns={\n",
    "        'average_score': 'chi2_score',\n",
    "        'support':       'chi2_support'\n",
    "    }), on='feature', how='left'\n",
    ").merge(\n",
    "    rocauc_threshold.stat_df[['feature', 'average_score', 'support']].rename(columns={\n",
    "        'average_score': 'roc_auc_score',\n",
    "        'support':       'roc_auc_support'\n",
    "    }), on='feature', how='left'\n",
    ").merge(\n",
    "    logistic_selector.stat_df[['feature', 'average_score', 'support']].rename(columns={\n",
    "        'average_score': 'logistic_score',\n",
    "        'support':       'logistic_support'\n",
    "    }), on='feature', how='left'\n",
    ").merge(\n",
    "    ridge_selector.stat_df[['feature', 'average_score', 'support']].rename(columns={\n",
    "        'average_score': 'ridge_score',\n",
    "        'support':       'ridge_support'\n",
    "    }), on='feature', how='left'\n",
    ").merge(\n",
    "    decision_tree_selector.stat_df[['feature', 'average_score', 'support']].rename(columns={\n",
    "        'average_score': 'decision_tree_score',\n",
    "        'support':       'decision_tree_support'\n",
    "    }), on='feature', how='left'\n",
    ").merge(\n",
    "    extra_tree_selector.stat_df[['feature', 'average_score', 'support']].rename(columns={\n",
    "        'average_score': 'extra_tree_score',\n",
    "        'support':       'extra_tree_support'\n",
    "    }), on='feature', how='left'\n",
    ").merge(\n",
    "    random_forest_selector.stat_df[['feature', 'average_score', 'support']].rename(columns={\n",
    "        'average_score': 'random_forest_score',\n",
    "        'support':       'random_forest_support'\n",
    "    }), on='feature', how='left'\n",
    ").merge(\n",
    "    xgboost_selector.stat_df[['feature', 'average_score', 'support']].rename(columns={\n",
    "        'average_score': 'xgboost_score',\n",
    "        'support':       'xgboost_support'\n",
    "    }), on='feature', how='left'\n",
    ").merge(\n",
    "    catboost_selector.stat_df[['feature', 'average_score', 'support']].rename(columns={\n",
    "        'average_score': 'catboost_score',\n",
    "        'support':       'catboost_support'\n",
    "    }), on='feature', how='left'\n",
    ")\n",
    "\n",
    "# Normalize score\n",
    "methods = [\n",
    "    'mutual_info', 'anova', 'chi2', 'roc_auc',\n",
    "    'logistic', 'ridge', 'decision_tree', 'extra_tree',\n",
    "    'random_forest', 'xgboost', 'catboost'\n",
    "]\n",
    "for method in methods:\n",
    "    feature_df[f'{method}_score'] = feature_df[f'{method}_score'] / feature_df[f'{method}_score'].sum()\n",
    "\n",
    "# Average score\n",
    "feature_df['average_score'] = feature_df[[f'{x}_score' for x in methods]].sum(axis=1) / len(methods)\n",
    "\n",
    "# Support ratio\n",
    "feature_df['support_ratio'] = feature_df[[f'{x}_support' for x in methods]].astype(int).sum(axis=1) / len(methods)\n",
    "\n",
    "# Support score\n",
    "feature_df['support_score'] = feature_df['average_score'] * feature_df['support_ratio']\n",
    "\n",
    "# Supported features\n",
    "feature_df['support'] = feature_df['support_score'] > feature_df['support_score'].mean()\n",
    "\n",
    "# Top N supported features\n",
    "top_k   = 20\n",
    "rank_df = feature_df.copy()\n",
    "rank_df['k_support'] = True\n",
    "rank_df.sort_values(by='support_score', ascending=False, inplace=True)\n",
    "rank_df = rank_df[['feature', 'k_support']][:top_k]\n",
    "\n",
    "feature_df = feature_df.merge(rank_df, on='feature', how='left')\n",
    "feature_df['k_support'].fillna(False, inplace=True)\n",
    "feature_df.to_excel(f'{OUT_PATH_FILE}feature_selection.xlsx', index=False)\n",
    "\n",
    "# Graph data\n",
    "feature_df.sort_values(by='feature', inplace=True)\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(\n",
    "            x=feature_df['feature'],\n",
    "            y=feature_df['support_score'],\n",
    "            marker_color=np.where(feature_df['support'], DEFAULT_PLOTLY_COLORS[2],\n",
    "                         np.where(feature_df['k_support'], DEFAULT_PLOTLY_COLORS[1], DEFAULT_PLOTLY_COLORS[3]))\n",
    "    )]\n",
    ")\n",
    "data_groups.append(fig['data'])\n",
    "subplot_titles.append(f'Top {top_k} Supported Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.datagroups_subplots(\n",
    "    data_groups,\n",
    "    max_col=2,\n",
    "    title='Phase 4 - Feature Selection - Classification',\n",
    "    out_path=OUT_PATH_GRAPH,\n",
    "    subplot_kwargs={\n",
    "        'subplot_titles': subplot_titles,\n",
    "        'vertical_spacing': .06\n",
    "    },\n",
    "    layout_kwargs={\n",
    "        'height': 1500,\n",
    "        'showlegend': False\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supported features\n",
    "# features = feature_df[feature_df['support']].sort_values(by='support_score', ascending=False)['feature'].values\n",
    "# features = ['Cons3_d2', 'Cons9_d4', '_Freq_0', 'Cons5_d1', 'Sp8_d2',\n",
    "#             'Cons10_d4', 'Cons4_d3', 'Cons9_d3', '_Freq_3', '3rd_d4',\n",
    "#             '1st_d1', '_Freq_77', '_Freq_2', 'Cons4_d2', '_Freq_6',\n",
    "#             'Sp7_d2', '_Freq_9', 'Sp5_d2', 'Sp2_d2', 'Sp3_d2',\n",
    "#             '1st_d4', 'Sp10_d3', 'Sp4_d2', 'Sp3_d4', 'Sp8_d4',\n",
    "#             'Cons9_d2', '_Freq_11', 'Sp6_d2', 'Cons1_d1', '_Freq_4',\n",
    "#             '_Freq_88', '_Freq_5', 'Cons2_d2', 'Cons8_d2', 'Sp4_d1',\n",
    "#             '_Freq_00', 'Sp7_d1', 'Sp5_d1', '_Freq_66', '2nd_d1',\n",
    "#             '1st_d3', 'Sp8_d1', '3rd_d1', '_Freq_7', 'Cons1_d2',\n",
    "#             '2nd_d3', 'Sp10_d1', 'Cons7_d2', 'Cons1_d3', 'Cons3_d4',\n",
    "#             'Cons1_d4', 'Sp6_d1', 'Cons10_d1', 'Sp7_d4', 'Cons5_d2',\n",
    "#             '_Freq_1', 'Sp2_d1', 'Cons6_d1', 'Sp4_d4', 'Cons6_d3',\n",
    "#             'Sp3_d3', 'Cons7_d3', '_Freq_55', 'Sp1_d4', 'Sp5_d4',\n",
    "#             'Cons2_d1', 'Sp6_d4', 'Sp7_d3', 'Sp4_d3', '1st_d2',\n",
    "#             'Cons2_d3', '_Freq_22', 'Cons7_d4', 'Cons3_d1', 'Cons3_d3',\n",
    "#             '_Freq_07', 'Cons6_d4', '_Freq_8', 'Cons8_d3', 'Cons4_d1',\n",
    "#             'Cons8_d1', 'Sp5_d3', 'Sp2_d4', '_Freq_36', 'Sp9_d3',\n",
    "#             '3rd_d2', 'Cons10_d2', '3rd_d3', '2nd_d4', 'Sp1_d2',\n",
    "#             '_Freq_35', 'Cons4_d4', '2nd_d2', 'Sp8_d3', 'Cons8_d4',\n",
    "#             'Sp1_d1', 'Sp1_d3', 'Cons10_d3', '_Freq_68', 'Sp9_d1']\n",
    "\n",
    "# Top N supported features\n",
    "# features = feature_df[feature_df['k_support']].sort_values(by='support_score', ascending=False)['feature'].values\n",
    "# features = ['Cons3_d2', 'Cons9_d4', '_Freq_0', 'Cons5_d1', 'Sp8_d2',\n",
    "#             'Cons10_d4', 'Cons4_d3', 'Cons9_d3', '_Freq_3', '3rd_d4',\n",
    "#             '1st_d1', '_Freq_77', '_Freq_2', 'Cons4_d2', '_Freq_6',\n",
    "#             'Sp7_d2', '_Freq_9', 'Sp5_d2', 'Sp2_d2', 'Sp3_d2']\n",
    "\n",
    "X_train = X_train[features].copy()\n",
    "X_test  = X_test[features].copy()\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5 - Feature Extraction\n",
    "- Separate dataset to features & target\n",
    "- Feature encoding\n",
    "- Ivis dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary crossentropy target\n",
    "target_onehot_encoder = DFOneHotEncoder(dtype='byte')\n",
    "y_train_binary = target_onehot_encoder.fit_transform(y_train.to_frame())\n",
    "\n",
    "# Sparse categorical crossentropy target\n",
    "y_train_sparse = y_train.copy()\n",
    "\n",
    "y_train_binary.shape, y_train_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "onehot_columns = [x for x in X_train.columns if any([x.startswith(y) for y in categories])]\n",
    "onehot_encoder = DFOneHotEncoder(columns=onehot_columns, dtype='byte', drop='first')\n",
    "\n",
    "minmax_scaler  = DFMinMaxScaler(columns=[x for x in X_train.columns if x not in onehot_columns])\n",
    "\n",
    "# TODO - define own model to prevent overfitting & explosive stacked_triplets_loss\n",
    "ivis_binary    = DFIvis(embedding_dims=2, epochs=100,\n",
    "                        k=15, n_epochs_without_progress=10, model='szubert',\n",
    "                        supervision_weight=1, supervision_metric='binary_crossentropy', distance='pn')\n",
    "\n",
    "ivis_sparse    = DFIvis(embedding_dims=2, epochs=100,\n",
    "                        k=15, n_epochs_without_progress=10, model='szubert',\n",
    "                        supervision_weight=1, supervision_metric='sparse_categorical_crossentropy', distance='pn')\n",
    "\n",
    "# Binary crossentropy\n",
    "steps = [\n",
    "    ('onehot_encoder', onehot_encoder),\n",
    "    ('minmax_scaler', minmax_scaler),\n",
    "    ('ivis_binary', ivis_binary),\n",
    "]\n",
    "ivis_binary_pipeline = Pipeline(steps, verbose=True)\n",
    "\n",
    "X_train_binary = ivis_binary_pipeline.fit_transform(X_train, y_train_binary)\n",
    "X_test_binary  = ivis_binary_pipeline.transform(X_test)\n",
    "\n",
    "X_train_binary.shape, X_test_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sparse categorical crossentropy\n",
    "steps = [\n",
    "    ('onehot_encoder', onehot_encoder),\n",
    "    ('minmax_scaler', minmax_scaler),\n",
    "    ('ivis_sparse', ivis_sparse),\n",
    "]\n",
    "ivis_sparse_pipeline = Pipeline(steps, verbose=True)\n",
    "\n",
    "X_train_sparse = ivis_sparse_pipeline.fit_transform(X_train, y_train_sparse)\n",
    "X_test_sparse  = ivis_sparse_pipeline.transform(X_test)\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names      = ['binary_loss', 'sparse_loss']\n",
    "losses     = [ivis_binary.model.loss_history_, ivis_sparse.model.loss_history_]\n",
    "max_length = max([len(x) for x in losses])\n",
    "\n",
    "tmp_df = pd.DataFrame([x+1 for x in range(max_length)], columns=['epoch'])\n",
    "for index, loss in enumerate(losses):\n",
    "    loss += [np.nan] * (max_length - len(loss))\n",
    "    tmp_df[names[index]] = loss\n",
    "\n",
    "vp.line(tmp_df,\n",
    "        xy_tuples=[('epoch', x) for x in names],\n",
    "        title='Phase 5 - Line - Ivis Loss',\n",
    "        out_path=OUT_PATH_GRAPH)\n",
    "\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title  = 'Phase 5 - Scatter - Ivis - Binary'\n",
    "tmp_df = pd.concat([X_train_binary, pd.Series(np.argmax(y_train_binary.values, axis=1), name=TARGET).astype(str)], axis=1)\n",
    "\n",
    "fig = px.scatter(\n",
    "    tmp_df.sort_values(by=TARGET),\n",
    "    x='ivis_0',\n",
    "    y='ivis_1',\n",
    "    color=TARGET,\n",
    "    marginal_x='histogram'\n",
    ")\n",
    "fig.update_layout(title=title)\n",
    "\n",
    "vp.generate_plot(fig,\n",
    "                 out_path=OUT_PATH_GRAPH,\n",
    "                 out_filename=title)\n",
    "\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title  = 'Phase 5 - Scatter - Ivis - Sparse Categorical'\n",
    "tmp_df = pd.concat([X_train_sparse, y_train_sparse.astype(str)], axis=1)\n",
    "\n",
    "fig = px.scatter(\n",
    "    tmp_df.sort_values(by=TARGET),\n",
    "    x='ivis_0',\n",
    "    y='ivis_1',\n",
    "    color=TARGET,\n",
    "    marginal_x='histogram'\n",
    ")\n",
    "fig.update_layout(title=title)\n",
    "\n",
    "vp.generate_plot(fig,\n",
    "                 out_path=OUT_PATH_GRAPH,\n",
    "                 out_filename=title)\n",
    "\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title  = 'Phase 5 - Scatter - Ivis - Binary (Test dataset)'\n",
    "tmp_df = pd.concat([X_test_binary, y_test.astype(str)], axis=1)\n",
    "\n",
    "fig = px.scatter(\n",
    "    tmp_df.sort_values(by=TARGET),\n",
    "    x='ivis_0',\n",
    "    y='ivis_1',\n",
    "    color=TARGET,\n",
    "    marginal_x='histogram'\n",
    ")\n",
    "fig.update_layout(title=title)\n",
    "\n",
    "vp.generate_plot(fig,\n",
    "                 out_path=OUT_PATH_GRAPH,\n",
    "                 out_filename=title)\n",
    "\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title  = 'Phase 5 - Scatter - Ivis - Sparse Categorical (Test dataset)'\n",
    "tmp_df = pd.concat([X_test_sparse, y_test.astype(str)], axis=1)\n",
    "\n",
    "fig = px.scatter(\n",
    "    tmp_df.sort_values(by=TARGET),\n",
    "    x='ivis_0',\n",
    "    y='ivis_1',\n",
    "    color=TARGET,\n",
    "    marginal_x='histogram'\n",
    ")\n",
    "fig.update_layout(title=title)\n",
    "\n",
    "vp.generate_plot(fig,\n",
    "                 out_path=OUT_PATH_GRAPH,\n",
    "                 out_filename=title)\n",
    "\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6 - Classification\n",
    "- Binary crossentropy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train & validation dataset with balanced target label\n",
    "train_df = balanced_target(pd.concat([X_train_binary, y_train], axis=1),\n",
    "                           target=TARGET, n_remain=350, random_state=10000)\n",
    "valid_df = balanced_target(pd.concat([X_train_binary, y_train], axis=1),\n",
    "                           target=TARGET, n_remain=150, random_state=10000, excludes=train_df.index)\n",
    "\n",
    "# Shuffle dataset\n",
    "train_df = train_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "valid_df = valid_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:')\n",
    "vp.value_count(train_df, TARGET)\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "vp.value_count(valid_df, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X_train_binary, y_train_binary = feature_target_split(train_df)\n",
    "X_valid_binary, y_valid_binary = feature_target_split(valid_df)\n",
    "\n",
    "# Categorical crossentropy target\n",
    "y_train_binary = target_onehot_encoder.transform(y_train_binary.to_frame())\n",
    "y_valid_binary = target_onehot_encoder.transform(y_valid_binary.to_frame())\n",
    "y_test_binary  = target_onehot_encoder.transform(y_test.to_frame())\n",
    "\n",
    "print('Train dataset:')\n",
    "print(X_train_binary.shape, y_train_binary.shape)\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "print(X_valid_binary.shape, y_valid_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "minmax_scaler  = DFMinMaxScaler()\n",
    "X_train_binary = minmax_scaler.fit_transform(X_train_binary)\n",
    "X_valid_binary = minmax_scaler.transform(X_valid_binary)\n",
    "X_test_binary  = minmax_scaler.transform(X_test_binary)\n",
    "\n",
    "X_train_binary.shape, X_valid_binary.shape, X_test_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(X):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=X.shape[1],\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    kernel_constraint=max_norm(5),\n",
    "                    use_bias=False))\n",
    "    model.add(BatchNormalization(scale=False,\n",
    "                                 renorm=True,\n",
    "                                 renorm_clipping={ 'rmax': 1, 'rmin': 0, 'dmax': 0 }))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=.2))\n",
    "    \n",
    "    model.add(Dense(64,\n",
    "                    kernel_initializer='he_uniform',\n",
    "                    kernel_constraint=max_norm(5),\n",
    "                    use_bias=False))\n",
    "    model.add(BatchNormalization(scale=False,\n",
    "                                 renorm=True,\n",
    "                                 renorm_clipping={ 'rmax': 1, 'rmin': 0, 'dmax': 0 }))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=.2))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Referecence: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "    metrics = [\n",
    "        CategoricalAccuracy(name='categorical_accuracy'),\n",
    "        AUC(name='auc'),\n",
    "        Precision(name='precision'),\n",
    "        Recall(name='recall'),\n",
    "    ]\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=.001, epsilon=.00001),\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "lrate = ReduceLROnPlateau(monitor='val_auc', factor=0.95, patience=15)\n",
    "\n",
    "# Reference: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=30, restore_best_weights=True)\n",
    "\n",
    "model   = compile_model(X_train_binary)\n",
    "history = model.fit(X_train_binary, y_train_binary,\n",
    "                    validation_data=(X_valid_binary, y_valid_binary),\n",
    "                    epochs=100,\n",
    "                    callbacks=[lrate, es],\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classif(y_true, y_pred):\n",
    "    cofmat_df = pd.DataFrame(confusion_matrix(y_true, y_pred))\n",
    "    cofmat_df.index.name   = 'True'\n",
    "    cofmat_df.columns.name = 'Pred'\n",
    "\n",
    "    print(cofmat_df)\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_test_binary.values, axis=1),\n",
    "    np.argmax(model.predict(X_test_binary), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_train_binary.values, axis=1),\n",
    "    np.argmax(model.predict(X_train_binary), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_valid_binary.values, axis=1),\n",
    "    np.argmax(model.predict(X_valid_binary), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['lr'],\n",
    "    mode='lines',\n",
    "    name='LR',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[-1]},\n",
    "))\n",
    "fig1 = go.Figure(data=data)\n",
    "\n",
    "# Loss\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['loss'],\n",
    "    mode='lines',\n",
    "    name='loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_loss'],\n",
    "    mode='lines',\n",
    "    name='val_loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig2 = go.Figure(data=data)\n",
    "\n",
    "# Accuracy\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='val_accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig3 = go.Figure(data=data)\n",
    "\n",
    "# AUC\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['auc'],\n",
    "    mode='lines',\n",
    "    name='auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_auc'],\n",
    "    mode='lines',\n",
    "    name='val_auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig4 = go.Figure(data=data)\n",
    "\n",
    "# Precision\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['precision'],\n",
    "    mode='lines',\n",
    "    name='precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_precision'],\n",
    "    mode='lines',\n",
    "    name='val_precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig5 = go.Figure(data=data)\n",
    "\n",
    "# Recall\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['recall'],\n",
    "    mode='lines',\n",
    "    name='recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_recall'],\n",
    "    mode='lines',\n",
    "    name='val_recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig6 = go.Figure(data=data)\n",
    "\n",
    "data_groups = [fig1['data'], fig2['data'], fig3['data'], fig4['data'], fig5['data'], fig6['data']]\n",
    "vp.datagroups_subplots(data_groups,\n",
    "                       max_col=3,\n",
    "                       title='Phase 6 - Metrics - Ivis - Binary',\n",
    "                       out_path=OUT_PATH_GRAPH,\n",
    "                       subplot_kwargs={\n",
    "                           'subplot_titles': ['Learning Rate', 'Loss', 'Accuracy', 'AUC', 'Precision', 'Recall']\n",
    "                       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 7 - Classification\n",
    "- Sparse categorical crossentropy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train & validation dataset with balanced target label\n",
    "train_df = balanced_target(pd.concat([X_train_sparse, y_train], axis=1),\n",
    "                           target=TARGET, n_remain=350, random_state=10000)\n",
    "valid_df = balanced_target(pd.concat([X_train_sparse, y_train], axis=1),\n",
    "                           target=TARGET, n_remain=150, random_state=10000, excludes=train_df.index)\n",
    "\n",
    "# Shuffle dataset\n",
    "train_df = train_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "valid_df = valid_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:')\n",
    "vp.value_count(train_df, TARGET)\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "vp.value_count(valid_df, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X_train_sparse, y_train_sparse = feature_target_split(train_df)\n",
    "X_valid_sparse, y_valid_sparse = feature_target_split(valid_df)\n",
    "\n",
    "# Categorical crossentropy target\n",
    "y_train_sparse = target_onehot_encoder.transform(y_train_sparse.to_frame())\n",
    "y_valid_sparse = target_onehot_encoder.transform(y_valid_sparse.to_frame())\n",
    "y_test_sparse  = target_onehot_encoder.transform(y_test.to_frame())\n",
    "\n",
    "print('Train dataset:')\n",
    "print(X_train_sparse.shape, y_train_sparse.shape)\n",
    "\n",
    "print('\\nValidate dataset:')\n",
    "print(X_valid_sparse.shape, y_valid_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "minmax_scaler  = DFMinMaxScaler()\n",
    "X_train_sparse = minmax_scaler.fit_transform(X_train_sparse)\n",
    "X_valid_sparse = minmax_scaler.transform(X_valid_sparse)\n",
    "X_test_sparse  = minmax_scaler.transform(X_test_sparse)\n",
    "\n",
    "X_train_sparse.shape, X_valid_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "lrate = ReduceLROnPlateau(monitor='val_auc', factor=0.95, patience=15)\n",
    "\n",
    "# Reference: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=30, restore_best_weights=True)\n",
    "\n",
    "model   = compile_model(X_train_sparse)\n",
    "history = model.fit(X_train_sparse, y_train_sparse,\n",
    "                    validation_data=(X_valid_sparse, y_valid_sparse),\n",
    "                    epochs=100,\n",
    "                    callbacks=[lrate, es],\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_test_sparse.values, axis=1),\n",
    "    np.argmax(model.predict(X_test_sparse), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_train_sparse.values, axis=1),\n",
    "    np.argmax(model.predict(X_train_sparse), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_valid_sparse.values, axis=1),\n",
    "    np.argmax(model.predict(X_valid_sparse), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['lr'],\n",
    "    mode='lines',\n",
    "    name='LR',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[-1]},\n",
    "))\n",
    "fig1 = go.Figure(data=data)\n",
    "\n",
    "# Loss\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['loss'],\n",
    "    mode='lines',\n",
    "    name='loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_loss'],\n",
    "    mode='lines',\n",
    "    name='val_loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig2 = go.Figure(data=data)\n",
    "\n",
    "# Accuracy\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='val_accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig3 = go.Figure(data=data)\n",
    "\n",
    "# AUC\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['auc'],\n",
    "    mode='lines',\n",
    "    name='auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_auc'],\n",
    "    mode='lines',\n",
    "    name='val_auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig4 = go.Figure(data=data)\n",
    "\n",
    "# Precision\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['precision'],\n",
    "    mode='lines',\n",
    "    name='precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_precision'],\n",
    "    mode='lines',\n",
    "    name='val_precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig5 = go.Figure(data=data)\n",
    "\n",
    "# Recall\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['recall'],\n",
    "    mode='lines',\n",
    "    name='recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_recall'],\n",
    "    mode='lines',\n",
    "    name='val_recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig6 = go.Figure(data=data)\n",
    "\n",
    "data_groups = [fig1['data'], fig2['data'], fig3['data'], fig4['data'], fig5['data'], fig6['data']]\n",
    "vp.datagroups_subplots(data_groups,\n",
    "                       max_col=3,\n",
    "                       title='Phase 7 - Metrics - Ivis - Sparse Categorical',\n",
    "                       out_path=OUT_PATH_GRAPH,\n",
    "                       subplot_kwargs={\n",
    "                           'subplot_titles': ['Learning Rate', 'Loss', 'Accuracy', 'AUC', 'Precision', 'Recall']\n",
    "                       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 8 - Classification\n",
    "- Binary + Sparse categorical crossentropy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combine = pd.concat([\n",
    "    X_train_binary.rename(columns={'ivis_0': 'ivis_binary_0', 'ivis_1': 'ivis_binary_1'}),\n",
    "    X_train_sparse.rename(columns={'ivis_0': 'ivis_sparse_0', 'ivis_1': 'ivis_sparse_1'}),\n",
    "], axis=1)\n",
    "\n",
    "X_valid_combine = pd.concat([\n",
    "    X_valid_binary.rename(columns={'ivis_0': 'ivis_binary_0', 'ivis_1': 'ivis_binary_1'}),\n",
    "    X_valid_sparse.rename(columns={'ivis_0': 'ivis_sparse_0', 'ivis_1': 'ivis_sparse_1'}),\n",
    "], axis=1)\n",
    "\n",
    "X_test_combine  = pd.concat([\n",
    "    X_test_binary.rename(columns={'ivis_0': 'ivis_binary_0', 'ivis_1': 'ivis_binary_1'}),\n",
    "    X_test_sparse.rename(columns={'ivis_0': 'ivis_sparse_0', 'ivis_1': 'ivis_sparse_1'}),\n",
    "], axis=1)\n",
    "\n",
    "X_train_combine.shape, X_valid_combine.shape, X_test_combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_combine = y_train_binary.copy()\n",
    "y_valid_combine = y_valid_binary.copy()\n",
    "y_test_combine  = y_test_binary.copy()\n",
    "\n",
    "y_train_combine.shape, y_valid_combine.shape, y_test_combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "lrate = ReduceLROnPlateau(monitor='val_auc', factor=0.95, patience=15)\n",
    "\n",
    "# Reference: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=30, restore_best_weights=True)\n",
    "\n",
    "model   = compile_model(X_train_combine)\n",
    "history = model.fit(X_train_combine, y_train_combine,\n",
    "                    validation_data=(X_valid_combine, y_valid_combine),\n",
    "                    epochs=100,\n",
    "                    callbacks=[lrate, es],\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_test_combine.values, axis=1),\n",
    "    np.argmax(model.predict(X_test_combine), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_train_combine.values, axis=1),\n",
    "    np.argmax(model.predict(X_train_combine), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set evaluation\n",
    "eval_classif(\n",
    "    np.argmax(y_valid_combine.values, axis=1),\n",
    "    np.argmax(model.predict(X_valid_combine), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['lr'],\n",
    "    mode='lines',\n",
    "    name='LR',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[-1]},\n",
    "))\n",
    "fig1 = go.Figure(data=data)\n",
    "\n",
    "# Loss\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['loss'],\n",
    "    mode='lines',\n",
    "    name='loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_loss'],\n",
    "    mode='lines',\n",
    "    name='val_loss',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig2 = go.Figure(data=data)\n",
    "\n",
    "# Accuracy\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_categorical_accuracy'],\n",
    "    mode='lines',\n",
    "    name='val_accuracy',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig3 = go.Figure(data=data)\n",
    "\n",
    "# AUC\n",
    "data = []\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['auc'],\n",
    "    mode='lines',\n",
    "    name='auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scattergl(\n",
    "    y=history.history['val_auc'],\n",
    "    mode='lines',\n",
    "    name='val_auc',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig4 = go.Figure(data=data)\n",
    "\n",
    "# Precision\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['precision'],\n",
    "    mode='lines',\n",
    "    name='precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_precision'],\n",
    "    mode='lines',\n",
    "    name='val_precision',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig5 = go.Figure(data=data)\n",
    "\n",
    "# Recall\n",
    "data = []\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['recall'],\n",
    "    mode='lines',\n",
    "    name='recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[0]},\n",
    "    legendgroup='train',\n",
    "))\n",
    "data.append(go.Scatter(\n",
    "    y=history.history['val_recall'],\n",
    "    mode='lines',\n",
    "    name='val_recall',\n",
    "    marker={'color': DEFAULT_PLOTLY_COLORS[1]},\n",
    "    legendgroup='validate',\n",
    "))\n",
    "fig6 = go.Figure(data=data)\n",
    "\n",
    "data_groups = [fig1['data'], fig2['data'], fig3['data'], fig4['data'], fig5['data'], fig6['data']]\n",
    "vp.datagroups_subplots(data_groups,\n",
    "                       max_col=3,\n",
    "                       title='Phase 8 - Metrics - Ivis - Binary + Sparse Categorical',\n",
    "                       out_path=OUT_PATH_GRAPH,\n",
    "                       subplot_kwargs={\n",
    "                           'subplot_titles': ['Learning Rate', 'Loss', 'Accuracy', 'AUC', 'Precision', 'Recall']\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
